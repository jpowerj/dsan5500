---
title: "Week 11: Parallel Pipelines and Map-Reduce"
subtitle: "*DSAN 5500: Data Structures, Objects, and Algorithms in Python*"
date: 2025-03-27
date-format: full
author: "Jeff Jacobs"
institute: "<a href='mailto:jj1088@georgetown.edu' target='_blank'>`jj1088@georgetown.edu`</a>"
lecnum: 11
bibliography: "../_DSAN5500.bib"
categories:
  - "Class Sessions"
format:
  revealjs:
    df-print: kable
    output-file: "slides.html"
    footer: "DSAN 5500 Week 11: {{< var w11.footer >}}"
    echo: true
    code-fold: show
    scrollable: true
    slide-number: true
    html-math-method: mathjax
    link-external-icon: true
    link-external-newwindow: true
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'>"
    theme: [default, "../dsan-globals/jjquarto.scss"]
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    revealjs-plugins:
      - simplemenu
  html:
    df-print: kable
    css: "../dsan-globals/jjstyles.css"
    output-file: "index.html"
    echo: true
    code-fold: show
    html-math-method: mathjax
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::


# Schedule {.smaller .small-title .crunch-title .crunch-callout .code-90 data-name="Schedule"}

Today's Planned Schedule:

| | Start | End | Topic |
|:- |:- |:- |:- |
| **Lecture** | 6:30pm | 7:30pm | [\[Embarrassingly\] Parallel Pipelines &rarr;](#serial-pipelines-rightarrow-parallel-pipelines) |
| | 7:30pm | 8:00pm | [Beyond Embarrassingly Parallel &rarr;](#beyond-embarrassingly-parallel-tasks...) |
| **Break!** | 8:00pm | 8:10pm | |
| | 8:10pm | 9:00pm | [Data Mining in Parallel &rarr;](#lab-time) |

# Serial Pipelines $\rightarrow$ Parallel Pipelines {data-stack-name="Parallel Pipelines"}

* Don't worry; for now, just a high-level overview of what we'll dive into in the final unit

## Quick Survey Question, for Intuition-Building

* Are humans capable of **"true" multi-tasking**?
  * As in, doing two things at the **exact same time?**
* (Or, do we instead **rapidly switch back and forth** between tasks?)

## The Answer {.smaller .crunch-title .crunch-ul}

* (From what we understand, at the moment, by way of studies in neuroscience/cognitive science/etc...)
* Humans are **not** capable of true multitasking! In CS terms, this would be called **multiprocessing** (more on this later)
* We **are** capable, however, of various modes of **concurrency**!

| | Multithreading | Asynchronous Execution |
| - | - | - |
| **Unconsciously**<br>(you do it already, "naturally") | Focus on **one** speaker within a loud room, with tons of other conversations entering your ears | Put something in oven, set alarm, go do something else, take out of oven once alarm goes off |
| **Consciously**<br>(you can do it with effort/practice) | Pat head (up and down) and rub stomach (circular motion) "simultaneously" | Throw a ball in the air, clap 3 times, catch ball |

## Helpful *Specifically* for Programming {.title-09}

* Course notes from MIT's [class on parallel computing](https://book.sciml.ai/) phrases it like: if implemented thoughtfully, concurrency is a **power multiplier** for your code (do 10 things in 1 second instead of 10 seconds...)

![](images/mario.gif){fig-align="center"}

## Helpful *In General* as a Way of Thinking! {.crunch-title .crunch-ul .title-08}

* Say you get hired as a Project Manager...
* Part of your job will fundamentally involve **pipelines!**
  * Need to know when Task $B$ does/does not require Task $A$ as a prerequisite
  * Need to know whether Task $A$ and Task $B$ can **share one resource** or **need their own individual resources**
  * Once Task $A$ and $B$ both complete, how do we **merge** their results together?

## Avoiding the Rabbithole

* Parallel computing is a **rabbithole**, but one you can safely avoid via simple heuristics ("rules of thumb")!

1. Check for optimizations to serial code first,
2. Check for **embarrassingly parallel** code blocks
3. Use **map-reduce** approach for more complicated cases

## "Embarrassingly Parallel" Pipelines {.smaller .crunch-title .crunch-ul}

* Technical definition: tasks within pipeline can easily be parallelized bc **no dependence** and **no need for communication** (see next slide). Better video explanation:

<center>

{{< video "images/triple_spatula.mp4" height="500" >}}

</center>

## Parallelizing Non-Embarrassingly-Parallel Pipelines {.smaller .title-10}

![epic_bacon_lifehack.jpeg](images/epic_bacon_lifehack.jpeg){fig-align="center"}

## Buzzkill: Complications to Come 😰 {.crunch-title .crunch-ul}

* If it's such a magical powerup, shouldn't we just **parallelize everything**? Answer: No 😞 because **overhead**.
* Overhead source 1: Beyond "embarrassingly parallel" cases, threads may require **their own separate stacks and heaps**
* Overhead source 2: Even after setting up new stacks and heaps, threads may need to **communicate with each other** (e.g. if they need to **synchronize** at some point(s))
* In fact, probably the earliest super-popular parallelization library was created to handle **Source 2**, not Source 1: <a href='https://en.wikipedia.org/wiki/Message_Passing_Interface' target='_blank'>**Message Passing Interface**</a> (C, C++, and Fortran)

## The Worst Part, IMO {.smaller}

* Plenty of problems in CS/data science have these kinds of complications... (if they weren't complicated, we wouldn't have as many jobs)
* We saw for example, with **hash tables**, how we can work to **minimize** collisions (MD5 and other provably-optimal hash functions), but can't **eliminate them** entirely
  * So, we tackle this complication by also developing efficient **collision-handling structures** like BSTs!
* With parallel overhead costs, however... I don't know of any easily-accessible "thing" like the theory of hash tables that can be used to optimize parallelization
  * In other words, you would think we could do a similar optimization: paralellize if benefits > costs, keep as serial otherwise
  * But, if you try to find a "framework" for this, you'll mostly find StackOverflow posts, textbooks, etc. which say "stuff varies too much between different chipsets, languages, operating systems, etc... sorry!"

## The Solution? {.crunch-title .crunch-ul}

* Again, as far as I can tell (despite workshops/courses and two summer internships just parallelizing stuff)...
* You just start trying to parallelize, carefully **measure** and **test** the performance gains/losses, and then
* Decide whether to commit to parallel or stick to serial, via an estimate of how your analysis/app will need to **scale!**
* Hence the usefulness of Prefect for visualizing tradeoff:
  * **Tasks** which used to run in serial will now run at same time, but will **take longer** (unless embarrassingly parallel) due to setup+communication overhead


## In Action {.crunch-title .crunch-details}

```{python}
#| code-fold: false
import time
from sympy.ntheory import factorint
from joblib import Parallel, delayed
parallel_runner = Parallel(n_jobs=4)
start, end = 500, 580
def find_prime_factors(num):
  time.sleep(.01)
  return factorint(num, multiple=True)
disp_time = lambda start, end: print('{:.4f} s'.format(end - start))
```

::: {.columns}
::: {.column width="50%"}

```{python}
serial_start = time.time()
result = [
  (i,find_prime_factors(i))
  for i in range(start, end+1)
]
serial_end = time.time()
disp_time(serial_start, serial_end)
```

:::
::: {.column width="50%"}

```{python}
par_start = time.time()
result = parallel_runner(
  delayed(find_prime_factors)(i)
  for i in range(start, end+1)
)
par_end = time.time()
disp_time(par_start, par_end)
```

:::
:::

# Beyond Embarrassingly Parallel Tasks... {data-stack-name="Beyond Embarrassingly Parallel"}

## What Happens When *Not* Embarrassingly Parallel? {.smaller .crunch-title .crunch-math .crunch-ul .title-09}

* Think of the difference between **linear** and **quadratic equations** in algebra:
* $3x - 1 = 0$ is "embarrassingly" solvable, on its own: you can solve it directly, by adding 3 to both sides $\implies x = \frac{1}{3}$. Same for $2x + 3 = 0 \implies x = -\frac{3}{2}$
* Now consider $6x^2 + 7x - 3 = 0$: Harder to solve "directly", so your instinct might be to turn to the laborious **quadratic equation**:

$$
\begin{align*}
x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{-7 \pm \sqrt{49 - 4(6)(-3)}}{2(6)} = \frac{-7 \pm 11}{12} = \left\{\frac{1}{3},-\frac{3}{2}\right\}
\end{align*}
$$

* And yet, $6x^2 + 7x - 3 = (3x - 1)(2x + 3)$, meaning that we could have **split** the problem into two "embarrassingly" solvable pieces, then **multiplied** to get result!

## The Analogy to Map-Reduce {.crunch-title .title-09}

| | |
|:- |:-:|
| $\leadsto$ If code is **not** embarrassingly parallel (instinctually requiring laborious serial execution), | $\underbrace{6x^2 + 7x - 3 = 0}_{\text{Solve using Quadratic Eqn}}$ |
| But can be **split** into... | $(3x - 1)(2x + 3) = 0$ |
| Embarrassingly parallel **pieces** which **combine** to same result, | $\underbrace{3x - 1 = 0}_{\text{Solve directly}}, \underbrace{2x + 3 = 0}_{\text{Solve directly}}$ |
| We can use **map-reduce** to achieve ultra speedup (running "pieces" on **GPU!**) | $\underbrace{(3x-1)(2x+3) = 0}_{\text{Solutions satisfy this product}}$ |

: {tbl-colwidths="[75,25]"}

## The Direct Analogy: Map-Reduce! {.smaller .crunch-title .crunch-details}

* Problem from DSAN 5000/5100: Computing SSR (**Sum of Squared Residuals**)
* $y = (1,3,2), \widehat{y} = (2, 5, 0) \implies \text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9$
* Computing pieces separately:

```python
map(do_something_with_piece, list_of_pieces)
```

```{python}
my_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])
map_result = list(my_map)
map_result
```

* Combining solved pieces

```python
reduce(how_to_combine_pair_of_pieces, pieces_to_combine)
```

```{python}
from functools import reduce
my_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)
my_reduce
```

# Functional Programming (FP){data-stack-name="Functional Programming"}

## Functions vs. Functionals {.crunch-title .crunch-ul .crunch-details .title-09 .crunch-li-8}

* You may have noticed: `map()` and `reduce()` are "meta-functions": functions that **take other functions as inputs**

::: {.columns}
::: {.column width="50%"}

```{python}
#| echo: true
#| code-fold: false
def add_5(num):
  return num + 5
add_5(10)
```

:::
::: {.column width="50%"}

```{python}
#| echo: true
#| code-fold: false
def apply_twice(fn, arg):
  return fn(fn(arg))
apply_twice(add_5, 10)
```

:::
:::

* In Python, functions **can be used as** vars (Hence `lambda`{.python}):

```{python}
#| echo: true
#| code-fold: false
add_5 = lambda num: num + 5
apply_twice(add_5, 10)
```

* This relates to a whole paradigm, "functional programming": mostly outside scope of course, but lots of important+useful takeaways/rules-of-thumb! &rarr;

## Train Your Brain for Functional Approach $\implies$ Master Debugging! {.crunch-title .title-09 .crunch-ul .crunch-blockquote}

* In CS Theory: enables <a href='https://softwarefoundations.cis.upenn.edu/vfa-current/toc.html' target='_blank'>formal proofs of correctness</a>
* In CS practice:

> When a program doesn’t work, each function is an interface point where you can check that the data are correct. You can look at the intermediate inputs and outputs to **quickly isolate the function** that's **responsible for a bug**.<br>(from Python's <a href='https://docs.python.org/3/howto/functional.html#ease-of-debugging-and-testing' target='_blank'>"Functional Programming HowTo"</a>)

## Code $\rightarrow$ Pipelines $\rightarrow$ *Debuggable* Pipelines {.smaller .crunch-title .title-11 .crunch-ul .crunch-p}

<!-- * Imperative ("standard") programming: Code runs line-by-line, from top to bottom -->
* Scenario: Run code, check the output, and... it's wrong 😵 what do you do?
* Usual approach: Read lines one-by-one, figuring out what they do, seeing if something **pops out** that seems wrong; adding comments like `# Convert to lowercase`{.python}

::: {.columns}
::: {.column width="50%"}

* **Easy case**: found typo in punctuation removal code. Fix the error, add comment like `# Remove punctuation`{.python}

    <i class='bi bi-arrow-return-right'></i> **Rule 1** of FP: transform these comments into **function names**

:::
::: {.column width="50%"}

* **Hard case**: Something in `load_text()` modifies a variable that **later on** breaks `remove_punct()` (Called a **side-effect**)

    <i class='bi bi-arrow-return-right'></i> **Rule 2** of FP: **NO SIDE-EFFECTS!**

:::
:::

```{dot}
//| echo: false
//| fig-height: 1
//| fig-cap: "*(Does this way of diagramming a program look familiar?)*"
digraph G {
  rankdir="TB";
	edge [
    penwidth=1.2
    arrowsize=0.85
  ];
  node [
    fontname="Courier"
    shape="plaintext"
  ];
  input[shape="plaintext", label="in.txt"];
  load_text[label=<
<table border="1" cellborder="0">
<tr>
  <td><font point-size="16">load_text</font></td>
</tr>
<tr>
  <td><font face="Arial" point-size="12">(Verb)</font></td>
</tr>
</table>
  >];
  lowercase[label=<
<table border="1" cellborder="0">
<tr>
  <td><font point-size="16">lowercase</font></td>
</tr>
<tr>
  <td><font face="Arial" point-size="12">(Verb)</font></td>
</tr>
</table>
  >];
  remove_punct[label=<
<table border="1" cellborder="0">
<tr>
  <td><font point-size="16">remove_punct</font></td>
</tr>
<tr>
  <td><font face="Arial" point-size="12">(Verb)</font></td>
</tr>
</table>
  >];
  remove_stopwords[label=<
<table border="1" cellborder="0">
<tr>
  <td><font point-size="16">remove_stopwords</font></td>
</tr>
<tr>
  <td><font face="Arial" point-size="12">(Verb)</font></td>
</tr>
</table>
  >];
  output[shape="plaintext", label="out.txt"];

  {
    rank=same;
    input -> load_text;
    load_text -> lowercase[label="🧐 ✅"];
    lowercase -> remove_punct[label="🧐 ✅"];
    remove_punct -> remove_stopwords[label="🧐 ❌❗️"];
    remove_stopwords -> output;
  }
}
```

* **With** side effects: ❌ $\implies$ issue is **somewhere earlier in the chain** 😩🏃‍♂️
* **No** side effects: ❌ $\implies$ issue **must be in `remove_punct()`!!!** 😎 <i class='bi bi-arrow-down'></i>⏱️ = <i class='bi bi-arrow-up'></i>💰

## If It's So Useful, Why Doesn't Everyone Do It? {.smaller .crunch-title .crunch-ul .crunch-p .crunch-quarto-figure .title-10}

* Trapped in **imperative** (sequential) coding mindset: **Path dependency** / QWERTY
* Reason **we** need to start thinking like this is: it's **1000x** harder to debug **parallel code!** So we need to be less ad hoc in how we write+debug, from here on out! 🙇‍♂️🙏

![From @leskovec_mining_2014](images/map_reduce_full.svg){fig-align="center" width="500"}

::: {.notes}

The title relates to a classic Economics joke (the best kind of joke): "An economist and a CEO are walking down the street, when the CEO points at the ground and tells the economist, 'look! A $20 bill on the ground!' The economist keeps on walking, scoffing at the CEO: 'don't be silly, if there was a $20 bill on the ground, somebody would have picked it up already'."

:::

# But... Why is All This Weird Mapping and Reducing Necessary? {data-stack-name="Map-Reduce Rationale"}

* Without knowing a bit more of the **internals** of **computing efficiency**, it may seem like a huge cost in terms of overly-complicated overhead, not to mention learning curve...

## The "Killer Application": Matrix Multiplication {.crunch-title .smaller .title-11}

* *(I learned from <a href='http://infolab.stanford.edu/~ullman/' target='_blank'>Jeff Ullman</a>, who did the obnoxious Stanford thing of mentioning in passing how "two previous students in the <a href='http://web.stanford.edu/class/cs246/' target='_blank'>class</a> did this for a cool final project on web crawling and, well, it escalated quickly", aka became Google)*

![From @leskovec_mining_2014, which is (legally) <a href='http://www.mmds.org/' target='_blank'>free online</a>!](images/map_reduce3.svg){fig-align="center"}

## The Killer Way-To-Learn: Text Counts! {.smaller .crunch-title .crunch-ul .crunch-p .title-12}

* [-@leskovec_mining_2014]: Text counts (2.2) $\rightarrow$ Matrix multiplication (2.3) $\rightarrow \cdots \rightarrow$ PageRank (5.1)
* *(And yall thought it was just busywork for HW3* 😏*)*
* The goal: User searches <a href='https://www.youtube.com/watch?v=eqwiC93zmxI' target='_blank'>"Denzel Curry"</a>... How **relevant** is a given webpage?
* Scenario 1: Entire internet fits on CPU $\implies$ We can just make a big big hash table:

```{dot}
//| echo: false
//| fig-height: 4
digraph G {
    rankdir="LR";
    nodesep="10";
	edge [penwidth=1.2,arrowsize=0.85]
    node [
        shape=plaintext;
        fontname="Courier";
    ];
    # Chunked
    internet [shape=none label=<
<table border="1" cellborder="0">
<tr>
  <td border="1" sides="b"><font face="Helvetica">Scan in O(n):</font></td>
</tr>
<tr>
  <td port="p1" align="text" balign="left">Today Denzel Washington<br />ate a big bowl of Yum's<br />curry. Denzel allegedly<br />rubbed his tum and said<br />"yum yum yum" when he<br />tasted today's curry.<br />"Yum! It is me Denzel,<br />curry is my fav!", he<br />exclaimed. According to<br />his friend Steph, curry<br />is indeed Denzel's fav.<br />We are live with Del<br />Curry in Washington for<br />a Denzel curry update.</td>
</tr>
</table>>];

    # Combined counts
    ccounts [shape=none label=<
<table border="1" cellborder="0">
<tr>
  <td border="1" sides="b" colspan="2"><font face="Helvetica">Overall Counts</font></td>
</tr>
<tr>
  <td port="p1" border="1" align="text" balign="left" sides="r">('according',1)<br />('allegedly',1)<br />('ate',1)<br />('big',1)<br />('bowl',1)<br />('curry',6)<br />('del',1)<br />('denzel',5)<br />('exclaimed',1)<br />('fav',2)<br />('friend',1)</td>
  <td align="text" balign="left">('indeed',1)<br />('live',1)<br />('rubbed',1)<br />('said',1)<br />('steph',1)<br />('tasted',1)<br />('today',2)<br />('tum',1)<br />('update',1)<br />('washington',2)<br />('yum',4)</td>
</tr>
</table>>];
    internet -> ccounts[label="Hash Table"];
}
```

## If Everything *Doesn't* Fit on CPU...

![From Cornell Virtual Workshop, <a href='https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/design' target='_blank'>"Understanding GPU Architecture"</a>](images/cpu_gpu.png){fig-align="center"}

## Break Problem into *Chunks* for the Green Bois! {.smaller .crunch-title .title-10 .crunch-quarto-figure}

```{dot}
//| echo: false
digraph G {
    rankdir="LR";
	edge [penwidth=1.2,arrowsize=0.85]
    node [
        shape=plaintext;
        fontname="Courier";
    ];
    # Chunked
    chunked [shape=none label=<
<table border="1" cellborder="0">
<tr>
  <td border="1" sides="b"><font face="Helvetica">Chunked Document</font></td>
</tr>
<tr>
  <td port="p1" border="1" align="text" balign="left" sides="b">Today Denzel Washington<br />ate a big bowl of Yum's<br />curry. Denzel allegedly<br />rubbed his tum and said</td>
</tr>
<tr>
  <td port="p2" border="1" align="text" balign="left" sides="b">"yum yum yum" when he<br />tasted today's curry.<br />"Yum! It is me Denzel,<br />curry is my fav!", he</td>
</tr>
<tr>
  <td port="p3" border="1" align="text" balign="left" sides="b">exclaimed. According to<br />his friend Steph, curry<br />is indeed Denzel's fav.<br />We are live with Del</td>
</tr>
<tr>
  <td port="p4" align="text" balign="left">Curry in Washington for<br />a Denzel curry update.</td>
</tr>
</table>>];
    // orig:p1 -> chunked:p1;
    // orig:p2 -> chunked:p2;
    // orig:p3 -> chunked:p3;
    // orig:p4 -> chunked:p4;

    # Chunked counts
    chcounts [shape=none label=<
<table border="1">
<tr>
  <td border="1" sides="b"><font face="Helvetica">Chunked Counts</font></td>
</tr>
<tr>
  <td port="p1" border="1" align="text" balign="left" sides="b">('today',1)<br />('denzel',1)<br />...<br />('tum',1)<br />('said',1)</td>
</tr>
<tr>
  <td port="p2" border="1" align="text" balign="left" sides="b">('yum',1)<br />('yum',1)<br />('yum',1)<br />...<br />('fav',1)</td>
</tr>
<tr>
  <td port="p3" border="1" align="text" balign="left" sides="b">('exclaimed',1)<br />...<br />('del',1)</td>
</tr>
<tr>
  <td port="p4" border="0" align="text" balign="left">('curry',1)<br />('washington',1)<br />('denzel',1)<br />('curry',1)<br />('update',1)</td>
</tr>
</table>>];
    chunked:p1 -> chcounts:p1[label="O(n/4)"];
    chunked:p2 -> chcounts:p2[label="O(n/4)"];
    chunked:p3 -> chcounts:p3[label="O(n/4)"];
    chunked:p4 -> chcounts:p4[label="O(n/4)"];

    # Sorted counts
    scounts [shape=none label=<
<table border="1">
<tr>
  <td border="1" sides="b"><font face="Helvetica">Hashed Counts</font></td>
</tr>
<tr>
  <td port="p1" border="1" align="text" balign="left" sides="b">('allegedly',1)<br />...<br />('curry',1)<br />('denzel',2)<br />...<br />('yum',1)</td>
</tr>
<tr>
  <td port="p2" border="1" align="text" balign="left" sides="b">('curry',2)<br />('denzel',1)<br />...<br />('yum',4)</td>
</tr>
<tr>
  <td port="p3" border="1" align="text" balign="left" sides="b">('according',1)<br />('curry',1)<br />('del',1)<br />('denzel',1)<br />...</td>
</tr>
<tr>
  <td port="p4" border="0" align="text" balign="left">('curry',2)<br />('denzel',1)<br />('update',1)<br />('washington',1)</td>
</tr>
</table>>];
    chcounts:p1 -> scounts:p1[label="O(n/4)"];
    chcounts:p2 -> scounts:p2[label="O(n/4)"];
    chcounts:p3 -> scounts:p3[label="O(n/4)"];
    chcounts:p4 -> scounts:p4[label="O(n/4)"];

    # Combined counts
    ccounts [shape=none,label=<
<table border="1" cellborder="0">
<tr>
  <td port="p0" border="1" sides="b"><font face="Helvetica">Overall Counts</font></td>
</tr>
<tr>
  <td port="p1" align="text" balign="left">('according',1)<br />('allegedly',1)<br />('ate',1)<br />('big',1)<br />('bowl',1)<br />('curry',6)<br />('del',1)<br />('denzel',5)<br />('exclaimed',1)<br />('fav',2)<br />('friend',1)<br />('indeed',1)<br />('live',1)<br />('rubbed',1)<br />('said',1)<br />('steph',1)<br />('tasted',1)<br />('today',2)<br />('tum',1)<br />('update',1)<br />('washington',2)<br />('yum',4)</td>
</tr>
</table>>];
    scounts:p1 -> ccounts:p1;
    scounts:p2 -> ccounts:p1;
    scounts:p3 -> ccounts:p1;
    scounts:p4 -> ccounts:p1;
    scounts:p2 -> ccounts[margin="5",labeldistance="4",penwidth="0",arrowsize="0.01",taillabel=<
<table border="0" cellborder="0">
<tr>
  <td bgcolor="white" color="black"><font color="black">Merge in<br />O(n)</font></td>
</tr>
</table>
    >,headlabel=" ",minlen="2"];
}
```

* $\implies$ Total = $O(3n) = O(n)$
* But **also** optimized in terms of constants, because of **sequential memory reads**

## HW5: Possibility 1

* Map-Reduced **Matrix-Vector Multiplication**

![<a href='https://www.walletfox.com/course/qtconcurrentmatrixvector.php' target='_blank'>Image source</a>](images/matvec2_img.png){fig-align="center"}

## HW5: Possibility 2 (More Likely?)

* Scrape quotes **in parallel!**
* Advantage: Builds on HW4
* Disadvantage: Embarrassingly parallel, so... no Map-Reduce coding practice
* (Probable tiebreaker: If you're interested in Map-Reduce, do **Matrix Multiplication** as **Final Project!**)

## References

::: {#refs}
:::
